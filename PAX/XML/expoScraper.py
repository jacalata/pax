
from urllib.request import urlopen, Request
from bs4 import BeautifulSoup
from xml.etree.ElementTree import Element, SubElement, Comment, tostring
from fileUtils import saveStringAsFile, saveBytesAsFile, getOutputFolder
from logging import debugPrint
import datetime
import sys
import getopt
import os
import string #for clearing weird characters out of strings for filenames
import time #sleep between requests


osPath = os.path.dirname(__file__)

def getExpoXML(TESTLOCALLY, DEBUG , DOWNLOAD, SHORTMODE, filesInfo):
    if (TESTLOCALLY):
        debugPrintLabel(DEBUG , "TestLocally")
        pageLocation = sampledatafolder+"\expolist.htm"
        page = open(pageLocation, encoding='utf-8')
    else:
        url = "https://m.guidebook.com/guide/48636/list/184670/" #PAX Aus 2015
        debugPrint(DEBUG , "expo url", url)
        sock = urlopen(url)
        page = sock.read() #returns a bytes object
        #save the page for offline work or debugging
        if DOWNLOAD:
            saveBytesAsFile(page, offlinefolder+"\expolist.html", DEBUG );
        sock.close()

    soup = BeautifulSoup(page, "html.parser")
    listing = soup.find('div', attrs={'class': 'content-pane'}) 

    root = Element('xml')
    root.set('version', '1.0')
    if not SHORTMODE:
        root.append(Comment('Generated by ElementTree in webScraper.py at ')) #generated_on))
        #makes life easier if we generate it without a timestamp for super basic testing
    schedule = SubElement(root, 'Schedule')
        
    booths=listing.findAll('a', attrs={'class': 'cell-content'})

    if SHORTMODE:
        print ("SHORTY")
        booths = [booths[0]]
        
    # for each booth, get the location and name
    for boothDetailInfo in booths:
        
        debugPrint(DEBUG , "boothDetail html", boothDetailInfo)

        time.sleep(3)            

        boothDetail = SubElement(schedule, 'boothDetail')
        # to get more details, go through to the url
        detailUrl = "http://m.guidebook.com" + boothDetailInfo['href']

        if (TESTLOCALLY):
            debugPrintLabel(DEBUG , "TestLocally")
            detailUrl = detailUrl.rsplit('/', 1)[1]
            detailUrl = os.path.join(offlinefolder, detailUrl + ".html")
            detailPage = open(detailUrl, encoding=paxEncoding)
        else:
            detailSock = urlopen(detailUrl)
            detailPage = detailSock.read()
            debugPrint(DEBUG , "detailUrl", detailUrl)
            if DOWNLOAD:
                saveBytesAsFile(detailPage,
                            os.path.join(offlinefolder, detailUrl+".html"),
                            DEBUG )    

        detailSoup = BeautifulSoup(detailPage, "html.parser")
        boothDetailName = detailSoup.find('header', attrs={'class', 'item'})
        
        debugPrint(DEBUG , "booth html: ", boothDetailName)
        actualName = boothDetailName.text.strip()
        debugPrint(DEBUG , "extracted location:", actualName)
        boothDetail.set('name', actualName)


        boothDetailDetail = detailSoup.find('div', attrs={'class', 'description'})
        debugPrint(DEBUG , "booth Location html", boothDetailDetail)

        boothDetailLocation=boothDetailDetail.text.strip()
        debugPrint(DEBUG , "booth location text", boothDetailLocation)
          
        boothDetail.set('location', boothDetailLocation)

            
    debugPrint(DEBUG , "expo xml", root)

    expoFilename = "expo.xml"

    xml_string = tostring(root).decode('utf-8')    
    debugPrint(DEBUG , "expo xml", xml_string)
    saveStringAsFile(xml_string, getOutputFolder(DEBUG )+ expoFilename, DEBUG )

    fileDetail = SubElement(filesInfo, 'file')
    fileDetail.set('type', 'xml')
    fileDetail.set('name', expoFilename);  

    print("expo scraper done")
    return filesInfo

