#PAX7 script for scraping the web schedule and turning it into an xml doc

from urllib.request import urlopen, Request
from bs4 import BeautifulSoup
from xml.etree.ElementTree import Element, SubElement, Comment, tostring
from fileUtils import saveStringAsFile, saveBytesAsFile, getOutputFolder
from expoScraper import getExpoXML
from logging import debugPrint
import datetime
import sys
import getopt
import os
import time #sleep between requests
import string #for clearing weird characters out of strings for filenames


osPath = os.path.dirname(__file__)
now = datetime.datetime.now()
generated_on = str(now)
year = now.year #doesn't get specified, I guess they expect you to know what year it is
dateString = now.strftime("%x")
dateInteger = now.strftime("%Y%m%d")

currentSchedule = "http://south.paxsite.com/schedule";

# pull the metadata straight out of the schedule where possible
Locations = []
Kinds = [] #"Tabletop", "Panel", "Show", "Contest", "Omegathon", "FreePlay", "D&D", "Social"]

def usage():
    print("open a command line and call python webScraper.py.")
    print("arguments:")
    print("d / --debug: include basic logging")
    print("l / --local: test without hitting the server. Must provide local input")
    print("s / --short: parse only the first event of each day, speed up test cycle")


class Del:
  def __init__(self, keep=string.ascii_letters+'\\'):
    self.comp = dict((ord(c),c) for c in keep)
  def __getitem__(self, k):
    return self.comp.get(k)

DD = Del()



def createContentsXML():
    contents = Element('xml');
    contents.set('version', '1.0')
    contents.append(Comment('Generated by ElementTree in webScraper.py at ' + generated_on))
    versionInfo = SubElement(contents, 'version')
    versionInfo.set('date', dateString)
    versionInfo.set('fileLocation', "http://paxwp7.nfshost.com/PAXEast/schedule.zip")
    versionInfo.set('number', dateInteger)
    return contents


def main(argv):
    print ("Arguments")
    print (argv)

    # set defaults
    TESTLOCALLY = False
    SHORTMODE = False
    DEBUG  = True
    DOWNLOAD = False
    DEBUGPRINT = DEBUG 


#todo - make debugmode an integer and give things levels of severity
    try:                                
        opts, args = getopt.getopt(argv, "dhlsvz:", ["debug", "help", "local", "short", "zdownload"])
    except getopt.GetoptError:          
        usage()                         
        sys.exit(2)
        
    for opt, arg in opts:               
        if opt in ("-d", "--debug"):
            DEBUG  = True
        elif opt in ("-h", "--help"):
            usage()
            sys.exit()
        elif opt in ("-l", "--local"):
            TESTLOCALLY = True    
        elif opt in ("-s", "--short"):
            SHORTMODE = True
        elif opt in ("-z", "--zdownload"):
            DOWNLOAD = True
    print ("Debug, Local, Short = ", DEBUG , TESTLOCALLY, SHORTMODE)      
    print("---------------------------------------------------")         


    sampledatafolder = os.path.join(osPath,"sampledata")    
    offlinefolder =  os.path.join(osPath, "offline")
    debugPrint(DEBUG , "local folder", sampledatafolder)

    if (TESTLOCALLY):
        if (DOWNLOAD):
            print("TestLocally")
        pageLocation = sampledatafolder+"\schedule.htm"
        page = open(pageLocation, encoding='utf-8')
    else:
        url = currentSchedule;
        if (DEBUG ):
            print(url)
        headers = { 'User-Agent' : 'Mozilla/5.0' }
        req = Request(url, None, headers);
        sock = urlopen(req);
        page = sock.read() #returns a bytes object
        #save the page for offline work or debugging
        if DOWNLOAD:
            saveBytesAsFile(page, offlinefolder+"\schedule.html", DEBUG );
        sock.close()

    contents = createContentsXML() 
    filesInfo = SubElement(contents, 'files') 

    soup = BeautifulSoup(page, "html.parser")

    # scrape exhibitor list 
    filesInfo = getExpoXML(TESTLOCALLY, DEBUG , DOWNLOAD, SHORTMODE, filesInfo);    
 

    #scrape event schedule
    #todo - pull this into another file, just pass back the updated contents object
    schedule = soup.find('ul', attrs={'id': 'schedule'}) 
    days = schedule.findAll('li', recursive=False) #list of 3 days 
    for day in days:

        root = Element('xml')
        root.set('version', '1.0')
        if not SHORTMODE:
            root.append(Comment('Generated by ElementTree in webScraper.py at ' + generated_on))
            #makes life easier if we generate it without a timestamp for super basic testing
        schedule = SubElement(root, 'Schedule')
        
        dayName = day['id']
        dayDate = day.find('h2', recursive=True).text 
        if (True):
            print("############"+dayName)
        
        timeblocks=day.findAll('div', 'timeBlock')
        if SHORTMODE:
            timeblocks = [timeblocks[0]]
            
        for timeblock in timeblocks:
            
            events = timeblock.findAll('li',  recursive=True)
            if SHORTMODE:
                events = [events[0]]

            debugPrint(DEBUG , "events html", events)
            
            # for each event, define name,kind,location,datetime,end,description
            for eventInfo in events:
                time.sleep(3)
                # time format is Sunday 9/2/2013 10:00 am
                eventTime = dayDate + "/" + str(year) + " " + timeblock.find('h3', 'time').text
                debugPrint(DEBUG , "event time", eventTime)
                

                event = SubElement(schedule, 'Event')
                event.set('datetime', eventTime)
                debugPrint(DEBUG , "event info", eventInfo)
                # to get more details, go through to the url
                detailUrl = eventInfo.find('a')['href']
                if (TESTLOCALLY):
                    debugPrintLabel(DEBUG , "TestLocally")
                    detailUrl = detailUrl.rsplit('/', 1)[1]
                    detailUrl = os.path.join(offlinefolder, detailUrl + ".html")
                    detailPage = open(detailUrl, encoding=paxEncoding)
                else:
                    

                    headers = { 'User-Agent' : 'Mozilla/5.0' }
                    req = Request(detailUrl, None, headers);
                    detailSock = urlopen(req);
                    detailPage = detailSock.read()
                    debugPrint(DEBUG , "detail url", detailUrl)
                    if DOWNLOAD:
                        saveBytesAsFile(detailPage,
                                    os.path.join(offlinefolder, detailUrl+".html"),
                                    DEBUG )    
                detailSoup = BeautifulSoup(detailPage, "html.parser")
                eventDetail = detailSoup.find('div', 'white')
                
                eventName=eventDetail.find('h2').text
                debugPrint(DEBUG , "Name = ", eventName)
                event.set('name', eventName)
                
                details = eventDetail.findAll('p')
                if (len(details) == 0):
                    eventDescription = ""
                else:
                     eventDescription = details[0].text
                if eventDetail.find('h4') is not None:
                    eventDescription = eventDescription + " -- " + eventDetail.find('h4').text + " " + details[1].text
                debugPrint(DEBUG , "description = ",  eventDescription)
                event.set('description', eventDescription)

                options = eventDetail.find('ul', 'meta')
                debugPrint(DEBUG , "details = ", options.text)
                if (options.find('li', 'con')):
                    endTimeIndex = 2 # concert info gets shoehorned in front of the datetime info
                    print("CON")
                else:
                    endTimeIndex = 1
                endTimeText = options.findAll('li', recursive=True)[endTimeIndex].text#eeewwww 
                debugPrint(DEBUG , "end time text", endTimeText)
                endTime = endTimeText.split()[4]
                debugPrint(DEBUG , "end time value", endTime)
                # need to go from 1:30AM to 1.30 AM
                endTime = endTime.replace("AM", " AM")
                endTime = endTime.replace("PM", " PM")

                eventEnd = str(dayDate) + "/" + str(year) + " " + str(endTime)
                debugPrint(DEBUG , "end = ", eventEnd)
                event.set('end', eventEnd)

                eventLocation = options.find('a', recursive=True).text
                debugPrint(DEBUG , "location= ", eventLocation)
                event.set('location', eventLocation)

                if eventLocation not in Locations:
                    Locations.append(eventLocation)
                
                eventKind="Panel" #default assumption. Special cases:
                if 'Concerts' in eventName:
                    eventKind = "Show"
                elif ('Tournament' in eventLocation)  or ('Tournament' in eventName):
                    eventKind = "Contest"
                elif 'Omegathon' in eventName:
                    eventKind = "Omegathon"
                debugPrint(DEBUG , "kind = ", eventKind)
                event.set('kind', eventKind)

                if eventKind not in Kinds:
                    Kinds.append(eventKind)

        dayData = tostring(root).decode('utf-8')
        debugPrint(DEBUG , "day xml", dayData)
        saveStringAsFile(dayData, getOutputFolder(DEBUG) + dayName+".xml", DEBUG )


        fileDetail = SubElement(filesInfo, 'file')
        fileDetail.set('type', 'xml')
        fileDetail.set('name', dayName + ".xml");      

    #end for day in days


    # write contents list to file
    contentsData = tostring(contents).decode('utf-8')
    saveStringAsFile(contentsData, getOutputFolder(DEBUG) + "contents.xml", DEBUG )

    print("done")

        
if __name__ == "__main__":
    main(sys.argv[1:])

